services:
  llama-swap:
    image: ghcr.io/mostlygeek/llama-swap:cuda
    container_name: llama-swap
    ports:
      - "9292:8080"
    user: "0:0"
    volumes:
      - ./models:/models
      - ./config.yaml:/app/config.yaml
    environment:
      - LLAMA_MODEL_PATH=/models
      - HF_HOME=/models
      - LLAMA_CACHE=/models
      - LLAMA_ARG_FLASH_ATTN=on
      # - CUDA_VISIBLE_DEVICES="0,1,2,3"
      - NVIDIA_VISIBLE_DEVICES=all
      # Optional: Set default model if needed, otherwise it auto-detects or is set via API
    
    gpus: all

    restart: unless-stopped

    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
